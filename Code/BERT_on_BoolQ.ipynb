{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2qfGB8k-MQA",
        "outputId": "ab8a64bd-9477-4b64-8fda-ed2c555daa0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/\")"
      ],
      "metadata": {
        "id": "sdKQse5t-btS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "# Load data\n",
        "train_path = './Boolqa/train.jsonl'\n",
        "dev_path = './Boolqa/dev.jsonl'\n",
        "test_path = './Boolqa/test.jsonl'\n",
        "\n",
        "train_data = load_jsonl(train_path)\n",
        "dev_data = load_jsonl(dev_path)\n",
        "test_data = load_jsonl(test_path)\n",
        "\n",
        "# Convert to DataFrames\n",
        "train_df = pd.DataFrame(train_data)\n",
        "dev_df = pd.DataFrame(dev_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Inspect\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Dev size:\", len(dev_df))\n",
        "print(\"Test size:\", len(test_df))\n",
        "print(\"\\nTrain columns:\", train_df.columns.tolist())\n",
        "print(\"Dev columns:\", dev_df.columns.tolist())\n",
        "print(\"Test columns:\", test_df.columns.tolist())\n",
        "print(\"\\nTrain sample:\\n\", train_df.iloc[0].to_dict())\n",
        "print(\"Dev sample:\\n\", dev_df.iloc[0].to_dict())\n",
        "print(\"Test sample:\\n\", test_df.iloc[0].to_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mahwkqOSd6JF",
        "outputId": "d649c23c-ac7a-4559-cd72-a1612e7cd3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 9427\n",
            "Dev size: 3270\n",
            "Test size: 3245\n",
            "\n",
            "Train columns: ['question', 'title', 'answer', 'passage']\n",
            "Dev columns: ['question', 'title', 'answer', 'passage']\n",
            "Test columns: ['question', 'title', 'passage']\n",
            "\n",
            "Train sample:\n",
            " {'question': 'do iran and afghanistan speak the same language', 'title': 'Persian language', 'answer': True, 'passage': 'Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}\n",
            "Dev sample:\n",
            " {'question': 'does ethanol take more energy make that produces', 'title': 'Ethanol fuel', 'answer': False, 'passage': \"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\"}\n",
            "Test sample:\n",
            " {'question': 'is the first series 20 euro note still legal tender', 'title': '20 euro note', 'passage': 'Until now there has been only one complete series of euro notes; however a new series, similar to the current one, is being released. The European Central Bank will, in due time, announce when banknotes from the first series lose legal tender status.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df, is_labeled=True):\n",
        "    df['question'] = df['question'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['passage'] = df['passage'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "    if is_labeled:\n",
        "        df = df.dropna(subset=['question', 'passage', 'answer'])\n",
        "        df = df[df['answer'].isin([True, False])]\n",
        "        df = df.drop_duplicates(subset=['question', 'passage'])\n",
        "    else:\n",
        "        df = df.dropna(subset=['question', 'passage'])\n",
        "        df = df.drop_duplicates(subset=['question', 'passage'])\n",
        "    return df\n",
        "\n",
        "train_df_clean = clean_data(train_df, is_labeled=True)\n",
        "dev_df_clean = clean_data(dev_df, is_labeled=True)\n",
        "test_df_clean = clean_data(test_df, is_labeled=False)\n",
        "\n",
        "# Check label distribution\n",
        "print(\"Train label distribution:\\n\", train_df_clean['answer'].value_counts(normalize=True))\n",
        "print(\"Dev label distribution:\\n\", dev_df_clean['answer'].value_counts(normalize=True))\n",
        "print(\"Train size after cleaning:\", len(train_df_clean))\n",
        "print(\"Dev size after cleaning:\", len(dev_df_clean))\n",
        "print(\"Test size after cleaning:\", len(test_df_clean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJlT2Uk-eD7P",
        "outputId": "b0a63d67-f21b-4e46-e78d-03e512d11d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train label distribution:\n",
            " answer\n",
            "True     0.623104\n",
            "False    0.376896\n",
            "Name: proportion, dtype: float64\n",
            "Dev label distribution:\n",
            " answer\n",
            "True     0.621713\n",
            "False    0.378287\n",
            "Name: proportion, dtype: float64\n",
            "Train size after cleaning: 9427\n",
            "Dev size after cleaning: 3270\n",
            "Test size after cleaning: 3245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_dataset(df, tokenizer, max_len=512, is_labeled=True):\n",
        "    questions = df['question'].tolist()\n",
        "    passages = df['passage'].tolist()\n",
        "    labels = df['answer'].tolist() if is_labeled else None\n",
        "\n",
        "    # Tokenize\n",
        "    encoded = tokenizer(\n",
        "        text=questions,\n",
        "        text_pair=passages,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoded['input_ids']\n",
        "    attention_masks = encoded['attention_mask']\n",
        "    labels = torch.tensor([1 if label else 0 for label in labels], dtype=torch.long) if labels is not None else None\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels) if labels is not None else TensorDataset(input_ids, attention_masks)\n",
        "\n",
        "    # Check truncation\n",
        "    truncated_count = 0\n",
        "    total_tokens_lost = 0\n",
        "    for q, p in zip(questions, passages):\n",
        "        full_encoding = tokenizer(q, p, add_special_tokens=True, truncation=False, return_tensors='pt')\n",
        "        full_length = full_encoding['input_ids'].shape[1]\n",
        "        if full_length > max_len:\n",
        "            truncated_count += 1\n",
        "            total_tokens_lost += full_length - max_len\n",
        "\n",
        "    print(f\"Truncated samples: {truncated_count}/{len(df)} ({truncated_count/len(df)*100:.2f}%)\")\n",
        "    if truncated_count > 0:\n",
        "        print(f\"Avg tokens lost per truncated sample: {total_tokens_lost/truncated_count:.2f}\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Preprocess\n",
        "train_dataset = preprocess_dataset(train_df_clean, tokenizer, is_labeled=True)\n",
        "dev_dataset = preprocess_dataset(dev_df_clean, tokenizer, is_labeled=True)\n",
        "test_dataset = preprocess_dataset(test_df_clean, tokenizer, is_labeled=False)\n",
        "\n",
        "# Validate\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Sample train entry:\", next(iter(train_dataset)))  # Unpack to check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KU_IkB7frmy",
        "outputId": "6412214d-5b01-4b52-dbb7-b85b438e6337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncated samples: 16/9427 (0.17%)\n",
            "Avg tokens lost per truncated sample: 129.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncated samples: 10/3270 (0.31%)\n",
            "Avg tokens lost per truncated sample: 137.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncated samples: 6/3245 (0.18%)\n",
            "Avg tokens lost per truncated sample: 82.17\n",
            "Train dataset size: 9427\n",
            "Dev dataset size: 3270\n",
            "Test dataset size: 3245\n",
            "Sample train entry: (tensor([  101,  2079,  4238,  1998,  7041,  3713,  1996,  2168,  2653,   102,\n",
            "         4723,  1006,  1013,   100,  1010,  1011,  1130, 29681,  2078,  1013,\n",
            "         1007,  1010,  2036,  2124,  2011,  2049,  2203, 16585,  2213,  2521,\n",
            "         5332,  1006,  1291, 25573, 17149, 29824, 24830,  2521,  5332,  1006,\n",
            "         1042, 29678, 23432, 29692, 29715,  5332, 23432,  1007,  1006,  4952,\n",
            "         1007,  1007,  1010,  2003,  2028,  1997,  1996,  2530,  7726,  4155,\n",
            "         2306,  1996, 11424,  1011,  7726,  3589,  1997,  1996, 11424,  1011,\n",
            "         2647,  2653,  2155,  1012,  2009,  2003,  3952,  5287,  1999,  4238,\n",
            "         1010,  7041,  1006,  3985,  2124,  2004, 18243,  2072,  2144,  3845,\n",
            "         1007,  1010,  1998, 23538,  1006,  3985,  2124,  2004, 11937,  4478,\n",
            "         3211,  2144,  1996,  3354,  3690,  1007,  1010,  1998,  2070,  2060,\n",
            "         4655,  2029,  7145,  2020,  4723,  3686,  8384,  1998,  2641,  2112,\n",
            "         1997,  3618,  4238,  1012,  2009,  2003,  2517,  1999,  1996,  4723,\n",
            "        12440,  1010,  1037,  6310,  8349,  1997,  1996,  5640,  5896,  1010,\n",
            "         2029,  2993,  7964,  2013,  1996, 28934, 12440,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), tensor(1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 8  # Safe for T4\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "dev_loader = DataLoader(dev_dataset, sampler=SequentialSampler(dev_dataset), batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Dev batches: {len(dev_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdwjRpdCgMLl",
        "outputId": "f7a9af25-f08a-48d0-f1b3-a46859655279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 1179\n",
            "Dev batches: 409\n",
            "Test batches: 406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, get_scheduler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Check initial weight\n",
        "initial_weight = model.bert.encoder.layer[0].attention.self.query.weight[0][0].item()\n",
        "print(f\"Initial weight: {initial_weight}\")\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Slightly lower LR\n",
        "num_training_steps = len(train_loader) * 4\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=200, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "best_f1 = 0.0\n",
        "best_epoch = 0\n",
        "for epoch in range(4):\n",
        "    print(f\"\\nEpoch {epoch + 1}/4\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        batch = [b.to(device) for b in batch]\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(f\"Step {step + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    dev_preds, dev_labels = [], []\n",
        "    total_dev_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_dev_loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            dev_preds.extend(preds.cpu().numpy())\n",
        "            dev_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_dev_loss = total_dev_loss / len(dev_loader)\n",
        "    accuracy = accuracy_score(dev_labels, dev_preds)\n",
        "    f1 = f1_score(dev_labels, dev_preds)\n",
        "    print(f\"Validation Loss: {avg_dev_loss:.4f}\")\n",
        "    print(f\"Dev Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Dev F1: {f1:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_epoch = epoch + 1\n",
        "        model.save_pretrained(\"/content/boolq_finetuned_bert_best\")\n",
        "        tokenizer.save_pretrained(\"/content/boolq_finetuned_bert_best\")\n",
        "    elif epoch - best_epoch >= 1:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}. Best F1: {best_f1:.4f}\")\n",
        "        break\n",
        "\n",
        "# Check final weight\n",
        "final_weight = model.bert.encoder.layer[0].attention.self.query.weight[0][0].item()\n",
        "print(f\"Final weight: {final_weight}\")\n",
        "print(f\"Weight changed: {abs(final_weight - initial_weight):.6f}\")\n",
        "\n",
        "if best_f1 > 0:\n",
        "    print(f\"Best model saved from epoch {best_epoch} with F1 {best_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dc5718f1b3914ef882fb6c740ab451e7",
            "f8250878cc224652833bc40b3c369166",
            "9c4bc0115977465290f823eabdd52e43",
            "d7c86dc92ddb406ca6e1f8375cffef82",
            "2c366b194574423eaf5725e55543bb73",
            "a672b043b29a4ee3bbb0b822ebc9d385",
            "1073eaa0cf8d4041941b913150eaf9b8",
            "1e1eea384cab41249d95bb8a601ee13f",
            "af70ce1026f54c06a15416ce7765e220",
            "4c286b581bb849dfaecec4be45643427",
            "28705491c89d4fd8a51a4f9a07a2c5fe"
          ]
        },
        "id": "25NpNaOAgNb8",
        "outputId": "7064cf5b-4bf9-4781-8928-c96b452be79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc5718f1b3914ef882fb6c740ab451e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weight: -0.01640571653842926\n",
            "\n",
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         | 50/1179 [00:34<12:51,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.6437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         | 100/1179 [01:10<13:07,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.7725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 150/1179 [01:47<12:55,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.7526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 200/1179 [02:24<11:58,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.6058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 250/1179 [03:00<11:32,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.7434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 300/1179 [03:37<10:49,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.5477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 350/1179 [04:14<10:09,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.6156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 400/1179 [04:50<09:33,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.5395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 450/1179 [05:27<08:58,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.7012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 500/1179 [06:04<08:21,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.7383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  47%|████▋     | 550/1179 [06:40<07:45,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.7326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████     | 600/1179 [07:17<07:07,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.5619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▌    | 650/1179 [07:54<06:30,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.5831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  59%|█████▉    | 700/1179 [08:30<05:54,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.6438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▎   | 750/1179 [09:07<05:18,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.5249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 800/1179 [09:44<04:40,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.7493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 850/1179 [10:20<04:02,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.5103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▋  | 900/1179 [10:57<03:25,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.6170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████  | 950/1179 [11:34<02:48,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.7311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▍ | 1000/1179 [12:10<02:12,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.6258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 1050/1179 [12:47<01:35,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.6581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  93%|█████████▎| 1100/1179 [13:24<00:58,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.6334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  98%|█████████▊| 1150/1179 [14:00<00:21,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.7371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1179/1179 [14:21<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.6499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 409/409 [01:34<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.6072\n",
            "Dev Accuracy: 0.6844\n",
            "Dev F1: 0.7689\n",
            "\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         | 50/1179 [00:36<13:59,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.5956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         | 100/1179 [01:13<13:17,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.7461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 150/1179 [01:50<12:41,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.7095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 200/1179 [02:26<11:59,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.4389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 250/1179 [03:03<11:25,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.4548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 300/1179 [03:40<10:46,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.9234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 350/1179 [04:16<10:14,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.4123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 400/1179 [04:53<09:33,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.3735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 450/1179 [05:29<08:59,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.4067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 500/1179 [06:06<08:21,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.5356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  47%|████▋     | 550/1179 [06:43<07:42,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.3456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████     | 600/1179 [07:19<07:06,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.4023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▌    | 650/1179 [07:56<06:31,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.7325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  59%|█████▉    | 700/1179 [08:33<05:54,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.4084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▎   | 750/1179 [09:09<05:16,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.7512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 800/1179 [09:46<04:40,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.2542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 850/1179 [10:23<04:03,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.7538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▋  | 900/1179 [11:00<03:26,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.6350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████  | 950/1179 [11:36<02:48,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.3252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▍ | 1000/1179 [12:13<02:12,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.7807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 1050/1179 [12:50<01:35,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.4634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  93%|█████████▎| 1100/1179 [13:27<00:58,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.5943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  98%|█████████▊| 1150/1179 [14:03<00:21,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.4695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1179/1179 [14:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.5408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 409/409 [01:34<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.5870\n",
            "Dev Accuracy: 0.6966\n",
            "Dev F1: 0.7524\n",
            "\n",
            "Epoch 3/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         | 50/1179 [00:36<13:56,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.1791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         | 100/1179 [01:13<13:17,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.2806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 150/1179 [01:50<12:38,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.3529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 200/1179 [02:26<12:06,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.1454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 250/1179 [03:03<11:24,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.1034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 300/1179 [03:40<10:48,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.3382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 350/1179 [04:16<10:14,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.5836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 400/1179 [04:53<09:37,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.3144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 450/1179 [05:30<09:00,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.6901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 500/1179 [06:07<08:22,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.4178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  47%|████▋     | 550/1179 [06:43<07:44,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.1787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████     | 600/1179 [07:20<07:07,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.6131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▌    | 650/1179 [07:57<06:29,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.8581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  59%|█████▉    | 700/1179 [08:33<05:51,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.5357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▎   | 750/1179 [09:10<05:18,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.9697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 800/1179 [09:47<04:41,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.7944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 850/1179 [10:23<04:02,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.5138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▋  | 900/1179 [11:00<03:25,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.2173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████  | 950/1179 [11:36<02:48,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.0666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▍ | 1000/1179 [12:13<02:11,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 1.2664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 1050/1179 [12:50<01:35,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.1109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  93%|█████████▎| 1100/1179 [13:26<00:58,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.0432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  98%|█████████▊| 1150/1179 [14:03<00:21,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.3690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1179/1179 [14:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.3805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 409/409 [01:34<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.7416\n",
            "Dev Accuracy: 0.7153\n",
            "Dev F1: 0.7893\n",
            "\n",
            "Epoch 4/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         | 50/1179 [00:36<13:57,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.1249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         | 100/1179 [01:13<13:14,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.7406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 150/1179 [01:49<12:35,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.0220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 200/1179 [02:26<12:02,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.0725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 250/1179 [03:03<11:27,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.0523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 300/1179 [03:40<10:53,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.4677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 350/1179 [04:16<10:13,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.7435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 400/1179 [04:53<09:34,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.0839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 450/1179 [05:30<08:59,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 500/1179 [06:06<08:25,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.0797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  47%|████▋     | 550/1179 [06:43<07:43,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.0354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████     | 600/1179 [07:19<07:07,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.0166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▌    | 650/1179 [07:56<06:28,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.4744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  59%|█████▉    | 700/1179 [08:33<05:52,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.0388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▎   | 750/1179 [09:09<05:16,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.0990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 800/1179 [09:46<04:38,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.3045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 850/1179 [10:23<04:03,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.4526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▋  | 900/1179 [10:59<03:25,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.6087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████  | 950/1179 [11:36<02:49,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▍ | 1000/1179 [12:12<02:12,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.5493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 1050/1179 [12:49<01:35,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.3210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  93%|█████████▎| 1100/1179 [13:26<00:58,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.2210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  98%|█████████▊| 1150/1179 [14:02<00:21,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.5262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1179/1179 [14:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 409/409 [01:34<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.9786\n",
            "Dev Accuracy: 0.7095\n",
            "Dev F1: 0.7700\n",
            "Final weight: -0.014194637537002563\n",
            "Weight changed: 0.002211\n",
            "Best model saved from epoch 3 with F1 0.7893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create a zip file of the model directory\n",
        "model_dir = \"/content/boolq_finetuned_bert_best\"\n",
        "zip_path = \"/content/boolq_finetuned_bert_best.zip\"\n",
        "shutil.make_archive(\"/content/boolq_finetuned_bert_best\", 'zip', model_dir)\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ILmQNQgI0NXF",
        "outputId": "bfa29d97-6cc0-45cf-bbd9-c760010e3477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5ea649f3-6ed4-4cf1-8173-abc7d60dc309\", \"boolq_finetuned_bert_best.zip\", 405697902)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the directory exists\n",
        "model_dir = \"/content/boolq_finetuned_bert_best\"\n",
        "if os.path.exists(model_dir):\n",
        "    print(f\"Directory {model_dir} exists!\")\n",
        "    print(\"Files in directory:\", os.listdir(model_dir))\n",
        "else:\n",
        "    print(f\"Directory {model_dir} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELyBKTV1MH3",
        "outputId": "4db49bed-0c0e-494f-bd41-df86b76b80fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory /content/boolq_finetuned_bert_best exists!\n",
            "Files in directory: ['model.safetensors', 'tokenizer_config.json', 'config.json', 'special_tokens_map.json', 'vocab.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Assuming `model` is still in memory (from your training cell)\n",
        "# If not, we’ll reload from the last state\n",
        "try:\n",
        "    model.save_pretrained(\"./Data/boolq_finetuned_bert_best\")\n",
        "    tokenizer.save_pretrained(\"./Data/boolq_finetuned_bert_best\")\n",
        "    print(\"Model and tokenizer manually saved to /Data/boolq_finetuned_bert_best\")\n",
        "except NameError:\n",
        "    print(\"Model variable not found. Let’s reload and save.\")\n",
        "\n",
        "# Verify again\n",
        "if os.path.exists(\"/boolq_finetuned_bert_best\"):\n",
        "    print(\"Directory now exists!\")\n",
        "    print(\"Files in directory:\", os.listdir(\"./boolq_finetuned_bert_best\"))\n",
        "else:\n",
        "    print(\"Failed to save the model. Let’s troubleshoot further.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXddSfPh1VKW",
        "outputId": "3d36e604-c9a0-4fe7-8ffa-02dbdf43d6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer manually saved to /Data/boolq_finetuned_bert_best\n",
            "Failed to save the model. Let’s troubleshoot further.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, get_scheduler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the best model (Epoch 3)\n",
        "model = BertForSequenceClassification.from_pretrained('./Data/boolq_finetuned_bert_best')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer with stronger regularization\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.1)  # Lower LR, higher weight decay\n",
        "num_training_steps = len(train_loader) * 3  # Train for 3 epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "best_f1 = 0.7893  # From Epoch 3\n",
        "best_epoch = 0\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nEpoch {epoch + 1}/3\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        batch = [b.to(device) for b in batch]\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(f\"Step {step + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    dev_preds, dev_labels = [], []\n",
        "    total_dev_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_dev_loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            dev_preds.extend(preds.cpu().numpy())\n",
        "            dev_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_dev_loss = total_dev_loss / len(dev_loader)\n",
        "    accuracy = accuracy_score(dev_labels, dev_preds)\n",
        "    f1 = f1_score(dev_labels, dev_preds)\n",
        "    print(f\"Validation Loss: {avg_dev_loss:.4f}\")\n",
        "    print(f\"Dev Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Dev F1: {f1:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_epoch = epoch + 1\n",
        "        model.save_pretrained(\"./Data/boolq_finetuned_bert_best_v2\")\n",
        "        tokenizer.save_pretrained(\"./Data/boolq_finetuned_bert_best_v2\")\n",
        "        print(f\"New best model saved with F1 {best_f1:.4f}\")\n",
        "    elif epoch - best_epoch >= 1:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}. Best F1: {best_f1:.4f}\")\n",
        "        break\n",
        "\n",
        "print(f\"Best F1: {best_f1:.4f} at epoch {best_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "wqClO_hXzdC_",
        "outputId": "ff55e47e-232f-4585-f94a-4a5aabf92ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▍         | 50/1179 [00:37<14:01,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.0281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|▊         | 100/1179 [01:14<13:13,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.0229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 150/1179 [01:51<12:50,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.0136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 200/1179 [02:28<12:02,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.0124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 250/1179 [03:04<11:30,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 300/1179 [03:41<10:53,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.0543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|██▉       | 350/1179 [04:18<10:11,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.0125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 400/1179 [04:55<09:32,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.7332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 450/1179 [05:31<08:57,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.1191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 500/1179 [06:08<08:21,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.2272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  47%|████▋     | 550/1179 [06:45<07:44,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.1435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     | 571/1179 [07:00<07:27,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-be7cbf81f7fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the original model directory exists\n",
        "print(\"Checking /content/boolq_finetuned_bert_best:\")\n",
        "if os.path.exists(\"./Data/boolq_finetuned_bert_best\"):\n",
        "    print(\"Found! Contents:\", os.listdir(\"./Data/boolq_finetuned_bert_best\"))\n",
        "else:\n",
        "    print(\"Not found in ./Data/\")\n",
        "\n",
        "# Try to save the current model (if still in memory)\n",
        "try:\n",
        "    from transformers import BertForSequenceClassification\n",
        "    model.save_pretrained(\"./Data/boolq_finetuned_bert_best_current\")\n",
        "    tokenizer.save_pretrained(\"./Data/boolq_finetuned_bert_best_current\")\n",
        "    print(\"Current model saved to ./Data/boolq_finetuned_bert_best_current\")\n",
        "except NameError:\n",
        "    print(\"Model variable not found. Unable to save current state.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Ges2CN3f6X",
        "outputId": "87dc423d-6233-48b2-9cfe-1ffe9b5cd4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking /content/boolq_finetuned_bert_best:\n",
            "Found! Contents: ['config.json', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt']\n",
            "Current model saved to ./Data/boolq_finetuned_bert_best_current\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and download cleaned DataFrames\n",
        "train_df_clean.to_csv('./Data/train_df_clean.csv', index=False)\n",
        "dev_df_clean.to_csv('./Data/dev_df_clean.csv', index=False)\n",
        "test_df_clean.to_csv('./Data/test_df_clean.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download('./Data/train_df_clean.csv')\n",
        "files.download('./Data/dev_df_clean.csv')\n",
        "files.download('./Data/test_df_clean.csv')\n",
        "print(\"Downloaded cleaned DataFrames.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "u3PNzqpy3xAD",
        "outputId": "db2c7f7f-f579-49ab-c356-cc878e499b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fb572a2a-dc93-4c11-ae47-b3e0d8d980d0\", \"train_df_clean.csv\", 6044484)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_deadff8f-bac6-4499-8d66-1892f121bcde\", \"dev_df_clean.csv\", 2071811)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6ac44a2e-14c5-42d1-9fbf-178bb4ce1c2f\", \"test_df_clean.csv\", 2044189)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded cleaned DataFrames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a project directory and copy files\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "!mkdir -p \"{drive_path}\"\n",
        "if os.path.exists(\"./Data/boolq_finetuned_bert_best\"):\n",
        "    !cp -r /content/boolq_finetuned_bert_best \"{drive_path}/\"\n",
        "if os.path.exists(\"./Data/boolq_finetuned_bert_best_current\"):\n",
        "    !cp -r ./Data/boolq_finetuned_bert_best_current \"{drive_path}/\"\n",
        "!cp ./Data/train_df_clean.csv \"{drive_path}/\"\n",
        "!cp ./Data/dev_df_clean.csv \"{drive_path}/\"\n",
        "!cp ./Data/test_df_clean.csv \"{drive_path}/\"\n",
        "\n",
        "print(f\"Files backed up to {drive_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm1E0t_g35Xq",
        "outputId": "200509f2-1477-4411-e410-02664f73c142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files backed up to /content/drive/My Drive/Colab Notebooks/BoolQ_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "if os.path.exists(\"/content/boolq_finetuned_bert_best\"):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"/content/boolq_finetuned_bert_best\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            input_ids, attention_mask = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    test_df_clean['predicted_answer'] = [bool(pred) for pred in test_preds]\n",
        "    print(\"Sample test predictions:\\n\", test_df_clean[['question', 'predicted_answer']].head())\n",
        "    test_df_clean.to_csv('/content/boolq_test_predictions_epoch3.csv', index=False)\n",
        "    files.download('/content/boolq_test_predictions_epoch3.csv')\n",
        "    !cp /content/boolq_test_predictions_epoch3.csv \"{drive_path}/\"\n",
        "    print(\"Test predictions saved and downloaded.\")\n",
        "else:\n",
        "    print(\"Original model not found to generate predictions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "C8RJZh7a4arS",
        "outputId": "6fe3a5e1-7f16-4b27-b1aa-5d3ab0add330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 406/406 [01:34<00:00,  4.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample test predictions:\n",
            "                                             question  predicted_answer\n",
            "0  is the first series 20 euro note still legal t...              True\n",
            "1  do the champions league winners get automatic ...              True\n",
            "2                  can a bull snake kill a small dog              True\n",
            "3                are all nba playoff games best of 7             False\n",
            "4  can i use my train ticket on the tram in manch...              True\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_89f8e227-f47d-42d2-bdff-d757e5debb32\", \"boolq_test_predictions_epoch3.csv\", 2061310)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions saved and downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "bvYV0fAbtd1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "\n",
        "# Load cleaned DataFrames from Google Drive\n",
        "train_df_clean = pd.read_csv(f\"{drive_path}/train_df_clean.csv\")\n",
        "dev_df_clean = pd.read_csv(f\"{drive_path}/dev_df_clean.csv\")\n",
        "test_df_clean = pd.read_csv(f\"{drive_path}/test_df_clean.csv\")\n",
        "print(\"Loaded cleaned DataFrames:\")\n",
        "print(\"Train shape:\", train_df_clean.shape)\n",
        "print(\"Dev shape:\", dev_df_clean.shape)\n",
        "print(\"Test shape:\", test_df_clean.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym7w0kJ0tdoS",
        "outputId": "f0614518-89a1-4f34-cad3-3f56a2ac9c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cleaned DataFrames:\n",
            "Train shape: (9427, 4)\n",
            "Dev shape: (3270, 4)\n",
            "Test shape: (3245, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define dataset class (same as before)\n",
        "class BoolQDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = str(self.data.iloc[idx]['question'])\n",
        "        passage = str(self.data.iloc[idx]['passage'])\n",
        "        label = 1 if self.data.iloc[idx]['answer'] else 0\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            passage,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation='longest_first',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4bszbZst0xL",
        "outputId": "6ca898c7-be5b-461d-a017-8cdb61893b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = BoolQDataset(train_df_clean, tokenizer)\n",
        "dev_dataset = BoolQDataset(dev_df_clean, tokenizer)\n",
        "test_dataset = BoolQDataset(test_df_clean, tokenizer)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "dev_loader = DataLoader(dev_dataset, sampler=SequentialSampler(dev_dataset), batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Dev batches: {len(dev_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Vm1_9ot1Hn",
        "outputId": "d1ec55c3-90bb-4c36-bf09-9201b92a70de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 1179\n",
            "Dev batches: 409\n",
            "Test batches: 406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model exists\n",
        "print(\"Checking /content/boolq_finetuned_bert_best:\")\n",
        "if os.path.exists(\"/content/boolq_finetuned_bert_best\"):\n",
        "    print(\"Found! Contents:\", os.listdir(\"/content/boolq_finetuned_bert_best\"))\n",
        "else:\n",
        "    print(\"Not found in /content/\")\n",
        "\n",
        "# Check Google Drive\n",
        "print(\"\\nChecking Google Drive:\")\n",
        "if os.path.exists(f\"{drive_path}/boolq_finetuned_bert_best\"):\n",
        "    print(\"Found in Drive! Copying to /content/...\")\n",
        "    !cp -r \"{drive_path}/boolq_finetuned_bert_best\" /content/\n",
        "    print(\"Copied to /content/boolq_finetuned_bert_best\")\n",
        "else:\n",
        "    print(\"Not found in Google Drive. You may need to upload manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OveFI_GLt_3W",
        "outputId": "ae2c888f-23ef-4738-d75d-5c1555ffeaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking /content/boolq_finetuned_bert_best:\n",
            "Found! Contents: ['special_tokens_map.json', 'tokenizer_config.json', 'vocab.txt', 'model.safetensors', 'config.json']\n",
            "\n",
            "Checking Google Drive:\n",
            "Found in Drive! Copying to /content/...\n",
            "Copied to /content/boolq_finetuned_bert_best\n"
          ]
        }
      ]
    }
  ]
}