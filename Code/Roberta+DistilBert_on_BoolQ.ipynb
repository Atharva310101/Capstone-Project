{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqiIsDj-2XUz",
        "outputId": "0f6546c6-87de-4eae-f033-cf360504bdaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the Google Drive path\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "print(\"Files in Google Drive:\")\n",
        "!ls \"{drive_path}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs0X7qI92-8C",
        "outputId": "3ca24fa1-760e-48ce-dbe4-e37b77448c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in Google Drive:\n",
            "boolq_finetuned_bert_best\t   boolq_test_predictions_epoch3.csv  train_df_clean.csv\n",
            "boolq_finetuned_bert_best_current  dev_df_clean.csv\n",
            "boolq_test_predictions.csv\t   test_df_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PQtR6gHd3IRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cleaned DataFrames from Google Drive\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "train_df_clean = pd.read_csv(f\"{drive_path}/train_df_clean.csv\")\n",
        "dev_df_clean = pd.read_csv(f\"{drive_path}/dev_df_clean.csv\")\n",
        "test_df_clean = pd.read_csv(f\"{drive_path}/test_df_clean.csv\")\n",
        "print(\"Loaded cleaned DataFrames:\")\n",
        "print(\"Train shape:\", train_df_clean.shape)\n",
        "print(\"Dev shape:\", dev_df_clean.shape)\n",
        "print(\"Test shape:\", test_df_clean.shape)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Define the BoolQDataset class\n",
        "class BoolQDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512, label_column='answer', is_test=False):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_column = label_column\n",
        "        self.is_test = is_test  # Flag to indicate if this is the test set\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = str(self.data.iloc[idx]['question'])\n",
        "        passage = str(self.data.iloc[idx]['passage'])\n",
        "\n",
        "        # Handle labels (only for train/dev, not for test)\n",
        "        if not self.is_test:\n",
        "            label = 1 if self.data.iloc[idx][self.label_column] else 0\n",
        "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        else:\n",
        "            label_tensor = torch.tensor(0, dtype=torch.long)  # Dummy label for test set (not used)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            passage,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation='longest_first',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': label_tensor\n",
        "        }\n",
        "\n",
        "# Load the tokenizer (already done, but included for completeness)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create datasets\n",
        "label_column = 'answer'  # Confirmed as the correct column name\n",
        "train_dataset = BoolQDataset(train_df_clean, tokenizer, label_column=label_column)\n",
        "dev_dataset = BoolQDataset(dev_df_clean, tokenizer, label_column=label_column)\n",
        "test_dataset = BoolQDataset(test_df_clean, tokenizer, label_column=label_column, is_test=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "dev_loader = DataLoader(dev_dataset, sampler=SequentialSampler(dev_dataset), batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Dev batches: {len(dev_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlWvVbmA3Ih9",
        "outputId": "63404d1f-cdc9-4e5e-adec-a33bb2edcca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cleaned DataFrames:\n",
            "Train shape: (9427, 4)\n",
            "Dev shape: (3270, 4)\n",
            "Test shape: (3245, 3)\n",
            "Train batches: 1179\n",
            "Dev batches: 409\n",
            "Test batches: 406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the model from Google Drive to Colab\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "!cp -r \"{drive_path}/boolq_finetuned_bert_best\" /content/\n",
        "\n",
        "# Verify the model files\n",
        "print(\"Model files in /content/boolq_finetuned_bert_best:\")\n",
        "!ls /content/boolq_finetuned_bert_best\n",
        "\n",
        "# Load the model\n",
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained('/content/boolq_finetuned_bert_best')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"BERT model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lhX6Vz63x5x",
        "outputId": "8327337d-2ddc-423f-9198-12881567e0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model files in /content/boolq_finetuned_bert_best:\n",
            "config.json  model.safetensors\tspecial_tokens_map.json  tokenizer_config.json\tvocab.txt\n",
            "BERT model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the column names of each DataFrame\n",
        "print(\"Train DataFrame columns:\", train_df_clean.columns)\n",
        "print(\"Dev DataFrame columns:\", dev_df_clean.columns)\n",
        "print(\"Test DataFrame columns:\", test_df_clean.columns)\n",
        "\n",
        "# Print a sample of each DataFrame\n",
        "print(\"\\nTrain DataFrame sample:\\n\", train_df_clean.head())\n",
        "print(\"\\nDev DataFrame sample:\\n\", dev_df_clean.head())\n",
        "print(\"\\nTest DataFrame sample:\\n\", test_df_clean.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BkPMUwO4Kqh",
        "outputId": "e9ce808b-882d-498b-cd9d-d937ba75a61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataFrame columns: Index(['question', 'title', 'answer', 'passage'], dtype='object')\n",
            "Dev DataFrame columns: Index(['question', 'title', 'answer', 'passage'], dtype='object')\n",
            "Test DataFrame columns: Index(['question', 'title', 'passage'], dtype='object')\n",
            "\n",
            "Train DataFrame sample:\n",
            "                                             question  \\\n",
            "0    do iran and afghanistan speak the same language   \n",
            "1  do good samaritan laws protect those who help ...   \n",
            "2  is windows movie maker part of windows essentials   \n",
            "3  is confectionary sugar the same as powdered sugar   \n",
            "4         is elder scrolls online the same as skyrim   \n",
            "\n",
            "                      title  answer  \\\n",
            "0          Persian language    True   \n",
            "1        Good Samaritan law    True   \n",
            "2       Windows Movie Maker    True   \n",
            "3            Powdered sugar    True   \n",
            "4  The Elder Scrolls Online   False   \n",
            "\n",
            "                                             passage  \n",
            "0  Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...  \n",
            "1  Good Samaritan laws offer legal protection to ...  \n",
            "2  Windows Movie Maker (formerly known as Windows...  \n",
            "3  Powdered sugar, also called confectioners' sug...  \n",
            "4  As with other games in The Elder Scrolls serie...  \n",
            "\n",
            "Dev DataFrame sample:\n",
            "                                             question  \\\n",
            "0   does ethanol take more energy make that produces   \n",
            "1             is house tax and property tax are same   \n",
            "2  is pain experienced in a missing body part or ...   \n",
            "3  is harry potter and the escape from gringotts ...   \n",
            "4  is there a difference between hydroxyzine hcl ...   \n",
            "\n",
            "                                        title  answer  \\\n",
            "0                                Ethanol fuel   False   \n",
            "1                                Property tax    True   \n",
            "2                                Phantom pain    True   \n",
            "3  Harry Potter and the Escape from Gringotts    True   \n",
            "4                                 Hydroxyzine    True   \n",
            "\n",
            "                                             passage  \n",
            "0  All biomass goes through at least some of thes...  \n",
            "1  Property tax or 'house tax' is a local tax on ...  \n",
            "2  Phantom pain sensations are described as perce...  \n",
            "3  Harry Potter and the Escape from Gringotts is ...  \n",
            "4  Hydroxyzine preparations require a doctor's pr...  \n",
            "\n",
            "Test DataFrame sample:\n",
            "                                             question  \\\n",
            "0  is the first series 20 euro note still legal t...   \n",
            "1  do the champions league winners get automatic ...   \n",
            "2                  can a bull snake kill a small dog   \n",
            "3                are all nba playoff games best of 7   \n",
            "4  can i use my train ticket on the tram in manch...   \n",
            "\n",
            "                           title  \\\n",
            "0                   20 euro note   \n",
            "1  2018–19 UEFA Champions League   \n",
            "2                      Bullsnake   \n",
            "3                   NBA playoffs   \n",
            "4       Manchester station group   \n",
            "\n",
            "                                             passage  \n",
            "0  Until now there has been only one complete ser...  \n",
            "1  The final will be played at the Wanda Metropol...  \n",
            "2  Bullsnakes are very powerful constrictors who ...  \n",
            "3  All rounds are best-of-seven series. Series ar...  \n",
            "4  The Manchester station group is a station grou...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BoolQDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512, label_column='answer', is_test=False):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_column = label_column\n",
        "        self.is_test = is_test  # Flag to indicate if this is the test set\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = str(self.data.iloc[idx]['question'])\n",
        "        passage = str(self.data.iloc[idx]['passage'])\n",
        "\n",
        "        # Handle labels (only for train/dev, not for test)\n",
        "        if not self.is_test:\n",
        "            label = 1 if self.data.iloc[idx][self.label_column] else 0\n",
        "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        else:\n",
        "            label_tensor = torch.tensor(0, dtype=torch.long)  # Dummy label for test set (not used)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            passage,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation='longest_first',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': label_tensor\n",
        "        }\n",
        "\n",
        "# Create datasets with the correct label column name\n",
        "# Replace 'answer' with the actual column name after inspecting the DataFrames\n",
        "label_column = 'answer'  # Update this based on the output of Step 1\n",
        "train_dataset = BoolQDataset(train_df_clean, tokenizer, label_column=label_column)\n",
        "dev_dataset = BoolQDataset(dev_df_clean, tokenizer, label_column=label_column)\n",
        "test_dataset = BoolQDataset(test_df_clean, tokenizer, label_column=label_column, is_test=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "dev_loader = DataLoader(dev_dataset, sampler=SequentialSampler(dev_dataset), batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Dev batches: {len(dev_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ5WzmRN4SOC",
        "outputId": "dac4326d-867d-432d-8ec6-cf1bb0815323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 1179\n",
            "Dev batches: 409\n",
            "Test batches: 406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Evaluate on the dev set\n",
        "model.eval()\n",
        "dev_preds, dev_labels = [], []\n",
        "total_dev_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dev_loader, desc=\"Evaluating BERT\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        total_dev_loss += outputs.loss.item()\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        dev_preds.extend(preds.cpu().numpy())\n",
        "        dev_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "avg_dev_loss = total_dev_loss / len(dev_loader)\n",
        "accuracy = accuracy_score(dev_labels, dev_preds)\n",
        "f1 = f1_score(dev_labels, dev_preds)\n",
        "\n",
        "print(f\"BERT Validation Loss: {avg_dev_loss:.4f}\")\n",
        "print(f\"BERT Dev Accuracy: {accuracy:.4f}\")\n",
        "print(f\"BERT Dev F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1mNiTDE50GB",
        "outputId": "49fe2c33-70f0-4e5c-d6bb-d359c8f0a2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating BERT:   1%|▏         | 6/409 [00:02<02:02,  3.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:   8%|▊         | 34/409 [00:09<01:23,  4.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  17%|█▋        | 70/409 [00:17<01:11,  4.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  29%|██▊       | 117/409 [00:27<01:02,  4.70it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  51%|█████▏    | 210/409 [00:47<00:42,  4.63it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  56%|█████▌    | 229/409 [00:51<00:41,  4.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  89%|████████▉ | 363/409 [01:22<00:10,  4.41it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  92%|█████████▏| 377/409 [01:25<00:07,  4.45it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  96%|█████████▋| 394/409 [01:29<00:03,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT:  97%|█████████▋| 398/409 [01:30<00:02,  4.19it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating BERT: 100%|██████████| 409/409 [01:33<00:00,  4.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Validation Loss: 0.7416\n",
            "BERT Dev Accuracy: 0.7153\n",
            "BERT Dev F1: 0.7893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set with BERT\n",
        "model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Predicting with BERT\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Add predictions to the test DataFrame\n",
        "test_df_clean['predicted_answer_bert'] = [bool(pred) for pred in test_preds]\n",
        "print(\"Sample BERT test predictions:\\n\", test_df_clean[['question', 'predicted_answer_bert']].head())\n",
        "test_df_clean.to_csv('/content/boolq_test_predictions_bert.csv', index=False)\n",
        "\n",
        "# Download the predictions\n",
        "from google.colab import files\n",
        "files.download('/content/boolq_test_predictions_bert.csv')\n",
        "\n",
        "# Save to Google Drive\n",
        "!cp /content/boolq_test_predictions_bert.csv \"{drive_path}/\"\n",
        "print(\"BERT test predictions saved and downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "f9R3rG0G4pZX",
        "outputId": "514549f9-0db2-42df-f4ab-ed2325932c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting with BERT:  18%|█▊        | 72/406 [00:17<01:16,  4.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with BERT:  24%|██▍       | 99/406 [00:23<01:09,  4.39it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with BERT:  35%|███▍      | 141/406 [00:33<01:00,  4.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with BERT:  39%|███▉      | 158/406 [00:37<00:57,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with BERT:  41%|████      | 165/406 [00:39<00:56,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with BERT:  82%|████████▏ | 333/406 [01:20<00:17,  4.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with BERT: 100%|██████████| 406/406 [01:37<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample BERT test predictions:\n",
            "                                             question  predicted_answer_bert\n",
            "0  is the first series 20 euro note still legal t...                   True\n",
            "1  do the champions league winners get automatic ...                   True\n",
            "2                  can a bull snake kill a small dog                   True\n",
            "3                are all nba playoff games best of 7                  False\n",
            "4  can i use my train ticket on the tram in manch...                   True\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e29cb029-5281-4332-a269-168916d13e41\", \"boolq_test_predictions_bert.csv\", 2061315)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT test predictions saved and downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load RoBERTa tokenizer and model\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "roberta_model.to(device)\n",
        "print(\"RoBERTa model loaded successfully.\")\n",
        "\n",
        "# Create datasets with RoBERTa tokenizer\n",
        "train_dataset_roberta = BoolQDataset(train_df_clean, roberta_tokenizer, label_column='answer')\n",
        "dev_dataset_roberta = BoolQDataset(dev_df_clean, roberta_tokenizer, label_column='answer')\n",
        "train_loader_roberta = DataLoader(train_dataset_roberta, sampler=RandomSampler(train_dataset_roberta), batch_size=8)\n",
        "dev_loader_roberta = DataLoader(dev_dataset_roberta, sampler=SequentialSampler(dev_dataset_roberta), batch_size=8)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(roberta_model.parameters(), lr=1e-5, weight_decay=0.1)\n",
        "num_training_steps = len(train_loader_roberta) * 3\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "CtMdnXsm4tWj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "5908c6f2e7294686ba8b93aa8f972d10",
            "c3e4c572455f45d5ac61ce3551f22993",
            "ad4ce8ad72544695847a40ff29d9b91e",
            "b0e62656563f497d9798c4b46768f12c",
            "04f0fe24f4b840e080f2e3457010367f",
            "ccb7c574a1f24ee89f32de4db42dd234",
            "8bfc366f009f412290662d20103d050b",
            "47f18c45c716478896ff6bce27de21da",
            "0bc9a2b01e464d30949b8987b2084435",
            "d047392f3e414eee86c5493eac66878f",
            "2b3a8e4f4b8f4710baf15cc551eedc93",
            "5bb160ec5fc34deb9b95720560a54acc",
            "f32e5a3eecfe43bb951c6762867dd13e",
            "2a5917dae0f34485a580637abb7d7513",
            "43292e7299de47d180910fdbf9b36790",
            "d53b236500714197a9d14d70b90b71d6",
            "8e7d97b00f3c4eea9fc5f483de8498bc",
            "46cc3c801a874793a734b35afe547407",
            "f0a36b9f3b06453b82436d15780275df",
            "3093605a591f4135834483908e761cf2",
            "f919cd2a4cd84aa08b4e38fb72a8ca12",
            "bbd194efa2a1442b9505f73c3c8be362",
            "d6abb6c083aa4d57b743ffb6ded2aec3",
            "a7c8480cb27743eca1641642a395ce91",
            "a44081a625f146feb8d5540fdb4a02a1",
            "3f6c1f130f8b46f18c4d1bc2e0b994be",
            "818016a09f9e41acbe82587994b1792c",
            "01ccb389202a452d9cb76c2c6439a261",
            "c2b54920312644dca661060a4f332ce0",
            "300390b165be40a4b5bb9e5c956a5839",
            "d4407bd42e724d80b53c986e8afe4419",
            "ecf31e96922b4ff1972c9a67deb5183c",
            "c475dac000314d55a889d5c4952190c2",
            "357f9547ab294b00be5eed3a1934f421",
            "31962d573bb64b3a8fd54ac14a74318b",
            "30a3cd7471c04e0ebde4f0f906451860",
            "6d82a2ef704f44bfa9f9e9ffbba95d95",
            "9df7be0edb104940b19eafd20edcad93",
            "7a68b9be5be44bcfb8b3b9110bd2af46",
            "487bd529b4aa4a05af4ff140014cc358",
            "33bd25e538884258a149b8e9859b864f",
            "ed28abd9d7fb40bbb2403585809c7243",
            "492b82140c954e879017c255da9a3ada",
            "021df3b1e4a04590940a723c829aa34f",
            "a5d8f0ba76a0430ab8c606c07524d100",
            "33d9f30a04ce42dcba6d3fef7ae2e89c",
            "a9a4fc7072114ed3ac7d7b061f89e3c8",
            "15bd7899fee14d9f8903f33eb44b1486",
            "9360a7bf71f6453dbcfeb7c83dd34331",
            "80dc8710842e4c39b9c7f5b6963b7d40",
            "ecb8bf1385e84205a34176a041854fab",
            "fe11c0756bf0412480ff2ac5bc94f312",
            "97186a796f704e11b05296f8f2327981",
            "2f48a650b62b411e909d6cfa4415f34d",
            "e110e2fc928340aaa7a47cb450eec464",
            "5f080d0f1e074c6aa62be0ee80fe93d6",
            "878af8892d5f4818b81f7d99b0ab77c7",
            "db635ee1bd84470abceb31b5ae8130a3",
            "ec6c5a2f9b094091b8118097a247e142",
            "4adaef91d31f4864960be89abdd95134",
            "8e461ede523a4416b001988de6190f08",
            "4b6e68abfbe64d9e9aecf84c508eb57a",
            "61d0c2cd810d451e85df5ab5ad97f4af",
            "46be4af831624a3692bf5ab44f22d499",
            "b571d2702d3745968c811ef4c94658dc",
            "72a8e43930344598b9e1a8c4795aed51"
          ]
        },
        "outputId": "c7cde205-238a-496a-d96e-44fc49f8b347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5908c6f2e7294686ba8b93aa8f972d10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bb160ec5fc34deb9b95720560a54acc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6abb6c083aa4d57b743ffb6ded2aec3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "357f9547ab294b00be5eed3a1934f421"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5d8f0ba76a0430ab8c606c07524d100"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f080d0f1e074c6aa62be0ee80fe93d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for RoBERTa\n",
        "best_f1 = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nRoBERTa Epoch {epoch + 1}/3\")\n",
        "    roberta_model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_loader_roberta, desc=\"Training RoBERTa\")):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = roberta_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(roberta_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(f\"Step {step + 1}/{len(train_loader_roberta)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader_roberta)\n",
        "    print(f\"RoBERTa Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate RoBERTa\n",
        "    roberta_model.eval()\n",
        "    dev_preds, dev_labels = [], []\n",
        "    total_dev_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader_roberta, desc=\"Evaluating RoBERTa\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = roberta_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_dev_loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            dev_preds.extend(preds.cpu().numpy())\n",
        "            dev_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_dev_loss = total_dev_loss / len(dev_loader_roberta)\n",
        "    accuracy = accuracy_score(dev_labels, dev_preds)\n",
        "    f1 = f1_score(dev_labels, dev_preds)\n",
        "    print(f\"RoBERTa Validation Loss: {avg_dev_loss:.4f}\")\n",
        "    print(f\"RoBERTa Dev Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"RoBERTa Dev F1: {f1:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_epoch = epoch + 1\n",
        "        roberta_model.save_pretrained(\"/content/boolq_finetuned_roberta_best\")\n",
        "        roberta_tokenizer.save_pretrained(\"/content/boolq_finetuned_roberta_best\")\n",
        "        print(f\"New best RoBERTa model saved with F1 {best_f1:.4f}\")\n",
        "        drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "        !cp -r /content/boolq_finetuned_roberta_best \"{drive_path}/\"\n",
        "    elif epoch - best_epoch >= 1:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}. Best F1: {best_f1:.4f}\")\n",
        "        break\n",
        "\n",
        "print(f\"RoBERTa Best F1: {best_f1:.4f} at epoch {best_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvD3SjkDyW9V",
        "outputId": "be42305f-98bd-40c5-b81f-d66dc13484ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RoBERTa Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   4%|▍         | 50/1179 [00:37<16:41,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.5936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   5%|▍         | 56/1179 [00:41<13:39,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   6%|▌         | 69/1179 [00:50<13:44,  1.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   6%|▌         | 70/1179 [00:51<13:40,  1.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   7%|▋         | 85/1179 [01:02<13:20,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   8%|▊         | 100/1179 [01:13<15:39,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.7439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  13%|█▎        | 150/1179 [01:50<15:02,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.7517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  17%|█▋        | 200/1179 [02:26<14:16,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.6731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  21%|██        | 250/1179 [03:03<13:28,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.6106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  25%|██▌       | 300/1179 [03:39<12:48,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.5823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  30%|██▉       | 350/1179 [04:15<12:03,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.7083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  32%|███▏      | 378/1179 [04:35<09:42,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  34%|███▍      | 400/1179 [04:52<11:21,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.7797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  36%|███▌      | 419/1179 [05:05<09:12,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  37%|███▋      | 437/1179 [05:18<09:01,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  38%|███▊      | 450/1179 [05:28<10:38,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.7668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  40%|███▉      | 470/1179 [05:42<08:34,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  40%|████      | 472/1179 [05:44<08:32,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  41%|████      | 480/1179 [05:49<08:27,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  42%|████▏     | 500/1179 [06:04<09:53,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.4870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  44%|████▎     | 515/1179 [06:15<08:03,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  47%|████▋     | 550/1179 [06:41<09:09,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.5934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  51%|█████     | 600/1179 [07:17<08:25,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.5344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  54%|█████▍    | 641/1179 [07:46<06:31,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  55%|█████▌    | 650/1179 [07:54<07:40,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.5232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  59%|█████▉    | 700/1179 [08:30<06:57,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.7270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  64%|██████▎   | 750/1179 [09:06<06:14,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.8535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  64%|██████▍   | 754/1179 [09:09<05:03,  1.40it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  68%|██████▊   | 800/1179 [09:43<05:29,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.3272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  70%|██████▉   | 822/1179 [09:58<04:20,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  72%|███████▏  | 850/1179 [10:19<04:46,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.8959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  76%|███████▋  | 900/1179 [10:55<04:03,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.4230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  81%|████████  | 950/1179 [11:32<03:19,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.3200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  85%|████████▍ | 1000/1179 [12:08<02:35,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.5862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  88%|████████▊ | 1041/1179 [12:37<01:40,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  89%|████████▉ | 1050/1179 [12:44<01:52,  1.15it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.4536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  93%|█████████▎| 1100/1179 [13:21<01:09,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.6626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  98%|█████████▊| 1150/1179 [13:57<00:25,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.3743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  98%|█████████▊| 1161/1179 [14:05<00:13,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa: 100%|██████████| 1179/1179 [14:17<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Average Training Loss: 0.6298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating RoBERTa:   1%|▏         | 6/409 [00:01<01:29,  4.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:   8%|▊         | 34/409 [00:07<01:18,  4.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  17%|█▋        | 70/409 [00:14<01:11,  4.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  29%|██▊       | 117/409 [00:24<01:02,  4.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  51%|█████▏    | 210/409 [00:44<00:42,  4.69it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  56%|█████▌    | 229/409 [00:48<00:37,  4.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  89%|████████▉ | 363/409 [01:16<00:10,  4.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  92%|█████████▏| 377/409 [01:19<00:06,  4.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  96%|█████████▋| 394/409 [01:23<00:03,  4.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  97%|█████████▋| 398/409 [01:24<00:02,  4.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa: 100%|██████████| 409/409 [01:26<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Validation Loss: 0.5470\n",
            "RoBERTa Dev Accuracy: 0.7275\n",
            "RoBERTa Dev F1: 0.8089\n",
            "New best RoBERTa model saved with F1 0.8089\n",
            "\n",
            "RoBERTa Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   3%|▎         | 36/1179 [00:25<14:03,  1.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   4%|▍         | 50/1179 [00:36<16:45,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.2057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   6%|▌         | 73/1179 [00:52<13:26,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   7%|▋         | 80/1179 [00:58<13:15,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   7%|▋         | 87/1179 [01:03<13:11,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   8%|▊         | 100/1179 [01:13<15:38,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.4031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  10%|█         | 119/1179 [01:26<12:42,  1.39it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  13%|█▎        | 150/1179 [01:49<15:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 1.0039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  17%|█▋        | 200/1179 [02:25<14:15,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.3725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  21%|██        | 250/1179 [03:02<13:31,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.7081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  25%|██▌       | 300/1179 [03:38<12:45,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.9580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  30%|██▉       | 350/1179 [04:14<12:04,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.5797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  34%|███▎      | 396/1179 [04:47<09:31,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  34%|███▍      | 400/1179 [04:51<11:20,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.4013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  36%|███▌      | 421/1179 [05:05<09:10,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  38%|███▊      | 450/1179 [05:27<10:34,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.3488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  42%|████▏     | 500/1179 [06:03<09:51,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.5247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  45%|████▌     | 536/1179 [06:29<07:46,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  47%|████▋     | 550/1179 [06:40<09:09,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.4978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  51%|█████     | 600/1179 [07:16<08:24,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.4377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  52%|█████▏    | 617/1179 [07:28<06:47,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  55%|█████▍    | 647/1179 [07:50<06:28,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  55%|█████▌    | 650/1179 [07:52<07:42,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.6056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  57%|█████▋    | 675/1179 [08:10<06:07,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  59%|█████▉    | 700/1179 [08:29<06:57,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.5615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  63%|██████▎   | 748/1179 [09:03<05:13,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  64%|██████▎   | 750/1179 [09:05<06:15,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.4103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  67%|██████▋   | 789/1179 [09:33<04:45,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  68%|██████▊   | 800/1179 [09:41<05:31,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.3658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  72%|███████▏  | 850/1179 [10:18<04:46,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.6013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  73%|███████▎  | 858/1179 [10:23<03:51,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  76%|███████▋  | 900/1179 [10:54<04:02,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.6371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  80%|████████  | 944/1179 [11:26<02:50,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  81%|████████  | 950/1179 [11:30<03:19,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.9132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  85%|████████▍ | 1000/1179 [12:07<02:36,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.3002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  87%|████████▋ | 1028/1179 [12:27<01:49,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  87%|████████▋ | 1031/1179 [12:29<01:47,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  89%|████████▉ | 1050/1179 [12:43<01:52,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.1498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  93%|█████████▎| 1100/1179 [13:19<01:08,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.1561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  98%|█████████▊| 1150/1179 [13:56<00:25,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.4702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa: 100%|██████████| 1179/1179 [14:16<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Average Training Loss: 0.4813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating RoBERTa:   1%|▏         | 6/409 [00:01<01:29,  4.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:   8%|▊         | 34/409 [00:07<01:18,  4.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  17%|█▋        | 70/409 [00:14<01:12,  4.68it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  29%|██▊       | 117/409 [00:24<01:01,  4.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  51%|█████▏    | 210/409 [00:44<00:41,  4.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  56%|█████▌    | 229/409 [00:48<00:37,  4.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  89%|████████▉ | 363/409 [01:16<00:09,  4.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  92%|█████████▏| 377/409 [01:19<00:06,  4.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  96%|█████████▋| 394/409 [01:23<00:03,  4.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  97%|█████████▋| 398/409 [01:24<00:02,  4.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa: 100%|██████████| 409/409 [01:26<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Validation Loss: 0.5021\n",
            "RoBERTa Dev Accuracy: 0.7697\n",
            "RoBERTa Dev F1: 0.8216\n",
            "New best RoBERTa model saved with F1 0.8216\n",
            "\n",
            "RoBERTa Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   4%|▍         | 50/1179 [00:36<16:40,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.9649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   5%|▍         | 55/1179 [00:39<13:38,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:   8%|▊         | 100/1179 [01:12<15:36,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.4145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:   9%|▉         | 109/1179 [01:18<12:55,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  12%|█▏        | 145/1179 [01:45<12:39,  1.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  13%|█▎        | 150/1179 [01:49<15:02,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.3412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  16%|█▌        | 183/1179 [02:12<12:03,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  17%|█▋        | 200/1179 [02:25<14:11,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.8209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  21%|██        | 247/1179 [02:59<11:19,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  21%|██        | 250/1179 [03:02<13:31,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.4482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  24%|██▎       | 280/1179 [03:23<10:51,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  25%|██▌       | 300/1179 [03:38<12:47,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.5162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  28%|██▊       | 330/1179 [03:59<10:17,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  30%|██▉       | 350/1179 [04:14<12:04,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.1344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  34%|███▎      | 397/1179 [04:48<09:27,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  34%|███▍      | 400/1179 [04:51<11:22,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.0653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  35%|███▌      | 415/1179 [05:01<09:14,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  38%|███▊      | 450/1179 [05:27<10:36,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.1232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  39%|███▉      | 465/1179 [05:38<08:38,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  40%|███▉      | 470/1179 [05:41<08:33,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  42%|████▏     | 500/1179 [06:03<09:54,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.1796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  47%|████▋     | 550/1179 [06:40<09:09,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.2298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  51%|█████     | 600/1179 [07:16<08:25,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.5482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  55%|█████▌    | 650/1179 [07:52<07:40,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.0832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  59%|█████▉    | 700/1179 [08:29<06:58,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.4413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  64%|██████▎   | 750/1179 [09:05<06:15,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.0698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  68%|██████▊   | 800/1179 [09:42<05:30,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 1.0637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  72%|███████▏  | 850/1179 [10:18<04:46,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.2304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  76%|███████▋  | 900/1179 [10:54<04:03,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.6582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  79%|███████▉  | 936/1179 [11:20<02:56,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  81%|████████  | 950/1179 [11:31<03:19,  1.15it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.1374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  85%|████████▍ | 1000/1179 [12:07<02:36,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.0917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  87%|████████▋ | 1026/1179 [12:25<01:50,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  89%|████████▉ | 1050/1179 [12:43<01:52,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.2394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  93%|█████████▎| 1100/1179 [13:20<01:09,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.0955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  95%|█████████▍| 1119/1179 [13:33<00:43,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa:  98%|█████████▊| 1150/1179 [13:56<00:25,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.2994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RoBERTa:  99%|█████████▉| 1171/1179 [14:11<00:05,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa: 100%|█████████▉| 1176/1179 [14:15<00:02,  1.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training RoBERTa: 100%|██████████| 1179/1179 [14:17<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Average Training Loss: 0.3822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating RoBERTa:   1%|▏         | 6/409 [00:01<01:28,  4.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:   8%|▊         | 34/409 [00:07<01:18,  4.79it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  17%|█▋        | 70/409 [00:14<01:10,  4.79it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  29%|██▊       | 117/409 [00:24<01:02,  4.69it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  51%|█████▏    | 210/409 [00:44<00:41,  4.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  56%|█████▌    | 229/409 [00:48<00:37,  4.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  89%|████████▉ | 363/409 [01:16<00:09,  4.70it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  92%|█████████▏| 377/409 [01:19<00:06,  4.68it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  96%|█████████▋| 394/409 [01:23<00:03,  4.79it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa:  97%|█████████▋| 398/409 [01:24<00:02,  4.79it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating RoBERTa: 100%|██████████| 409/409 [01:26<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Validation Loss: 0.5660\n",
            "RoBERTa Dev Accuracy: 0.7835\n",
            "RoBERTa Dev F1: 0.8261\n",
            "New best RoBERTa model saved with F1 0.8261\n",
            "RoBERTa Best F1: 0.8261 at epoch 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load DistilBERT tokenizer and model\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "distilbert_model.to(device)\n",
        "print(\"DistilBERT model loaded successfully.\")\n",
        "\n",
        "# Create datasets with DistilBERT tokenizer\n",
        "train_dataset_distilbert = BoolQDataset(train_df_clean, distilbert_tokenizer, label_column='answer')\n",
        "dev_dataset_distilbert = BoolQDataset(dev_df_clean, distilbert_tokenizer, label_column='answer')\n",
        "train_loader_distilbert = DataLoader(train_dataset_distilbert, sampler=RandomSampler(train_dataset_distilbert), batch_size=8)\n",
        "dev_loader_distilbert = DataLoader(dev_dataset_distilbert, sampler=SequentialSampler(dev_dataset_distilbert), batch_size=8)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(distilbert_model.parameters(), lr=1e-5, weight_decay=0.1)\n",
        "num_training_steps = len(train_loader_distilbert) * 3\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "a78da463326443d88a661091561c9a65",
            "a3e8e4ad6e5943748ffc465a5b1312a5",
            "c5252a30caf44c1283d2e2994fd1b9a7",
            "2646d2f96c474e6eb05dfdcfba845252",
            "6fa208e73225495fbb6af38dd78a2550",
            "4b96034b63c34b008c2abe87a5f0a603",
            "a61010cc37ce426b95cb87e97cf797d3",
            "853a253bd1cf4b71af5d87314f8daa4a",
            "52eafc49c073467eacc78d145318d793",
            "2b4f7bcaaf034db5b593f6bb3a6cbdcf",
            "5a1c0e65606f4534ad2376397bcba3df",
            "48330ff3529843548a5b4784e38402ed",
            "309a9d804c164ecca745e24586a9153b",
            "3eac57d1ccfe4fc0adefe99d7e0b71c7",
            "d2df206dad534d029533e6d09c5270eb",
            "e3f65a1f4a334c388e89d9e600183b1b",
            "8e12e82229544d33af601d23b3fb76f9",
            "4947de9bb9cd401ea1c2cb3705970b81",
            "9cc3c4198774432e93c1ffb436c27435",
            "5978e0469d9245e28c82ea37c6e5ee85",
            "2044892abd9d4eb6848dfae524ad3f51",
            "22403d080d18418eaaa3dc18e8f94dea",
            "f52a0ed5f0f7428fb5823466fa1e6187",
            "7251119ec4734789a97a091659372c2a",
            "91517a4b8c4e46ba96018d7d6d5b5576",
            "1677c6da64e24417a7bdebf485621dbf",
            "bf7c853bc0354262988dd14df41d01a3",
            "d09022fd71bc42ecb9afa01e7bd51e55",
            "bb35a3a17c4d49b0a8735ed7b039bea7",
            "331c8987225647d4bfdbedd1ce7fa634",
            "dbf4702ecb9543df8b05b5bcd38c3ee7",
            "f7d87ad320434fd38beec437dd7ed72e",
            "88a4f5b674204d13b7aa239769805816",
            "e28b41ccff7a48dca26f07310bbaf39f",
            "0e9be4e2d80b4c4dbee6b2d02409aa6e",
            "b823fe30b5534415b5e315ae6fd5a7fb",
            "d48a34ecdfec45189513b446e7a2b9a6",
            "75b7a28aec9446178cb4ca83a230a1df",
            "8cfe21b11575433484a3075c617cf424",
            "baadf9d1cf15464eb335dd6caf317d47",
            "281616e212514f079ada26b9e4a2f119",
            "6bcd0990accf45758a5a764378f90278",
            "c0571a8525fc406cb16ef6e4788ba025",
            "f721f74d632c44b3b392c7de7612ccf2",
            "7017740b6ec849dcbfa977b9a9f10e92",
            "a907b030bcda4dfa844502dc7a12ef85",
            "70b1904f57504c30847c5e59559a7bd6",
            "ee2ffe6532f74125a3d099a1d215315d",
            "40f8d6b7a8b04ae487f22152413e9d78",
            "44c9b023471d4ddc9b82a6ee7a071f59",
            "bd95ade48ed3412fa18e1f92418576eb",
            "cad38811fb5b4734ab93a6ea225d25ef",
            "44b776f9d6ca4d439dfed1ddf5c5ed26",
            "64e16aed8dc048849b115acbb6c3d721",
            "a651f5788bc045b182198c423eee9c5b"
          ]
        },
        "id": "r7p8_a0H-3DP",
        "outputId": "67e27491-4a5b-4beb-c014-af4d45e53490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a78da463326443d88a661091561c9a65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48330ff3529843548a5b4784e38402ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f52a0ed5f0f7428fb5823466fa1e6187"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e28b41ccff7a48dca26f07310bbaf39f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7017740b6ec849dcbfa977b9a9f10e92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for DistilBERT\n",
        "best_f1 = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nDistilBERT Epoch {epoch + 1}/3\")\n",
        "    distilbert_model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_loader_distilbert, desc=\"Training DistilBERT\")):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(distilbert_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(f\"Step {step + 1}/{len(train_loader_distilbert)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader_distilbert)\n",
        "    print(f\"DistilBERT Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate DistilBERT\n",
        "    distilbert_model.eval()\n",
        "    dev_preds, dev_labels = [], []\n",
        "    total_dev_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader_distilbert, desc=\"Evaluating DistilBERT\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_dev_loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            dev_preds.extend(preds.cpu().numpy())\n",
        "            dev_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_dev_loss = total_dev_loss / len(dev_loader_distilbert)\n",
        "    accuracy = accuracy_score(dev_labels, dev_preds)\n",
        "    f1 = f1_score(dev_labels, dev_preds)\n",
        "    print(f\"DistilBERT Validation Loss: {avg_dev_loss:.4f}\")\n",
        "    print(f\"DistilBERT Dev Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"DistilBERT Dev F1: {f1:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_epoch = epoch + 1\n",
        "        distilbert_model.save_pretrained(\"/content/boolq_finetuned_distilbert_best\")\n",
        "        distilbert_tokenizer.save_pretrained(\"/content/boolq_finetuned_distilbert_best\")\n",
        "        print(f\"New best DistilBERT model saved with F1 {best_f1:.4f}\")\n",
        "        drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "        !cp -r /content/boolq_finetuned_distilbert_best \"{drive_path}/\"\n",
        "    elif epoch - best_epoch >= 1:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}. Best F1: {best_f1:.4f}\")\n",
        "        break\n",
        "\n",
        "print(f\"DistilBERT Best F1: {best_f1:.4f} at epoch {best_epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZttdOc6N-_Rv",
        "outputId": "fcba5c1d-8b84-4981-eefe-21a15be0c6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DistilBERT Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:   4%|▍         | 51/1179 [00:17<06:16,  3.00it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.6614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:   7%|▋         | 77/1179 [00:27<06:32,  2.81it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:   9%|▊         | 101/1179 [00:35<06:06,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.5987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  13%|█▎        | 151/1179 [00:53<06:01,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.6358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  17%|█▋        | 201/1179 [01:12<05:44,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.6119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  21%|██▏       | 251/1179 [01:30<05:22,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.4710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  24%|██▍       | 284/1179 [01:42<05:23,  2.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  25%|██▍       | 290/1179 [01:44<05:21,  2.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  26%|██▌       | 301/1179 [01:48<05:01,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.6605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  30%|██▉       | 351/1179 [02:06<04:45,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.6686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  33%|███▎      | 388/1179 [02:20<04:48,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  34%|███▍      | 401/1179 [02:24<04:33,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.7649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  36%|███▋      | 429/1179 [02:35<04:33,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  38%|███▊      | 451/1179 [02:43<04:12,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.5938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  42%|████▏     | 501/1179 [03:01<03:55,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.8544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  47%|████▋     | 551/1179 [03:19<03:36,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.6795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  51%|█████     | 601/1179 [03:37<03:21,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.6842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  51%|█████▏    | 607/1179 [03:40<03:27,  2.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  54%|█████▍    | 641/1179 [03:52<03:16,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  55%|█████▌    | 651/1179 [03:56<03:05,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.4664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  59%|█████▉    | 701/1179 [04:14<02:45,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.6352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  64%|██████▎   | 751/1179 [04:32<02:28,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.5602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  64%|██████▍   | 753/1179 [04:33<02:31,  2.81it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  65%|██████▍   | 765/1179 [04:37<02:30,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  68%|██████▊   | 801/1179 [04:50<02:10,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.6391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  68%|██████▊   | 807/1179 [04:52<02:15,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  69%|██████▉   | 812/1179 [04:54<02:13,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  72%|███████▏  | 851/1179 [05:08<01:54,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.6042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  76%|███████▋  | 901/1179 [05:27<01:37,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.5132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  78%|███████▊  | 920/1179 [05:34<01:34,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  81%|████████  | 951/1179 [05:45<01:19,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.5821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  85%|████████▍ | 998/1179 [06:02<01:05,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  85%|████████▍ | 1001/1179 [06:03<01:02,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.7982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  88%|████████▊ | 1034/1179 [06:15<00:52,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  89%|████████▉ | 1051/1179 [06:21<00:44,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.6528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  93%|█████████▎| 1101/1179 [06:40<00:27,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.5181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  96%|█████████▌| 1132/1179 [06:51<00:17,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  98%|█████████▊| 1151/1179 [06:58<00:09,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.8120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT: 100%|██████████| 1179/1179 [07:08<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT Average Training Loss: 0.6386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating DistilBERT:   1%|▏         | 6/409 [00:00<00:50,  7.98it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:   8%|▊         | 34/409 [00:04<00:48,  7.72it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  17%|█▋        | 70/409 [00:08<00:40,  8.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  29%|██▊       | 117/409 [00:14<00:34,  8.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  51%|█████▏    | 210/409 [00:25<00:23,  8.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  56%|█████▌    | 229/409 [00:28<00:22,  7.89it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  89%|████████▉ | 363/409 [00:44<00:05,  8.22it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  92%|█████████▏| 377/409 [00:46<00:03,  8.26it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  96%|█████████▋| 394/409 [00:48<00:01,  8.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  97%|█████████▋| 398/409 [00:49<00:01,  8.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT: 100%|██████████| 409/409 [00:50<00:00,  8.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT Validation Loss: 0.6073\n",
            "DistilBERT Dev Accuracy: 0.6612\n",
            "DistilBERT Dev F1: 0.7247\n",
            "New best DistilBERT model saved with F1 0.7247\n",
            "\n",
            "DistilBERT Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:   4%|▍         | 51/1179 [00:18<06:33,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.7046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:   7%|▋         | 87/1179 [00:31<06:37,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:   9%|▊         | 101/1179 [00:36<06:15,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.7914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  11%|█▏        | 133/1179 [00:48<06:21,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  13%|█▎        | 151/1179 [00:54<05:56,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.4932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  16%|█▋        | 193/1179 [01:10<05:58,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  17%|█▋        | 201/1179 [01:13<05:39,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.6407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  21%|██▏       | 251/1179 [01:31<05:29,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.4654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  26%|██▌       | 301/1179 [01:49<05:11,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.7053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  30%|██▉       | 351/1179 [02:07<04:48,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.6463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  33%|███▎      | 394/1179 [02:23<04:46,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  34%|███▍      | 401/1179 [02:26<04:30,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 0.5189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  36%|███▋      | 428/1179 [02:35<04:33,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  38%|███▊      | 451/1179 [02:44<04:11,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.4192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  42%|████▏     | 501/1179 [03:02<03:57,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.2942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  47%|████▋     | 551/1179 [03:20<03:38,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.2467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  51%|█████     | 601/1179 [03:39<03:19,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.7051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  55%|█████▌    | 651/1179 [03:57<03:01,  2.90it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.5364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  57%|█████▋    | 676/1179 [04:06<03:03,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  59%|█████▉    | 701/1179 [04:15<02:45,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.5550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  61%|██████▏   | 724/1179 [04:23<02:44,  2.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  64%|██████▎   | 751/1179 [04:33<02:28,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.6969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  68%|██████▊   | 801/1179 [04:51<02:10,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.2875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  72%|███████▏  | 851/1179 [05:10<01:53,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.5658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  76%|███████▋  | 901/1179 [05:28<01:36,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.4977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  77%|███████▋  | 908/1179 [05:30<01:38,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  81%|████████  | 951/1179 [05:46<01:18,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.6803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  82%|████████▏ | 966/1179 [05:52<01:17,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  85%|████████▍ | 1001/1179 [06:04<01:02,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.1843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  89%|████████▉ | 1051/1179 [06:22<00:44,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.6343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  90%|█████████ | 1062/1179 [06:26<00:42,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  91%|█████████ | 1067/1179 [06:28<00:40,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  93%|█████████▎| 1098/1179 [06:40<00:29,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  93%|█████████▎| 1101/1179 [06:41<00:27,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.4190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  95%|█████████▍| 1120/1179 [06:48<00:21,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  95%|█████████▌| 1121/1179 [06:48<00:21,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  98%|█████████▊| 1151/1179 [06:59<00:09,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.4444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  99%|█████████▉| 1169/1179 [07:05<00:03,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT: 100%|██████████| 1179/1179 [07:09<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT Average Training Loss: 0.5515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating DistilBERT:   1%|▏         | 6/409 [00:00<00:51,  7.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:   8%|▊         | 34/409 [00:04<00:44,  8.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  17%|█▋        | 70/409 [00:08<00:40,  8.39it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  29%|██▊       | 117/409 [00:14<00:35,  8.18it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  51%|█████▏    | 210/409 [00:25<00:26,  7.64it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  56%|█████▌    | 229/409 [00:28<00:21,  8.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  89%|████████▉ | 363/409 [00:44<00:05,  8.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  92%|█████████▏| 377/409 [00:46<00:03,  8.27it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  96%|█████████▋| 394/409 [00:48<00:01,  8.32it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  97%|█████████▋| 398/409 [00:48<00:01,  8.26it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT: 100%|██████████| 409/409 [00:50<00:00,  8.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT Validation Loss: 0.5804\n",
            "DistilBERT Dev Accuracy: 0.6954\n",
            "DistilBERT Dev F1: 0.7586\n",
            "New best DistilBERT model saved with F1 0.7586\n",
            "\n",
            "DistilBERT Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:   4%|▍         | 51/1179 [00:18<06:34,  2.86it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50/1179, Loss: 0.3968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:   9%|▊         | 101/1179 [00:36<06:17,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/1179, Loss: 0.5046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  10%|█         | 122/1179 [00:44<06:25,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  13%|█▎        | 151/1179 [00:54<05:57,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 150/1179, Loss: 0.1937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  15%|█▌        | 178/1179 [01:04<06:05,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  16%|█▋        | 192/1179 [01:09<06:00,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  17%|█▋        | 201/1179 [01:13<05:38,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200/1179, Loss: 0.6753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  21%|██▏       | 251/1179 [01:31<05:28,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 250/1179, Loss: 0.4934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  26%|██▌       | 301/1179 [01:49<05:04,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 300/1179, Loss: 0.4900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  30%|██▉       | 351/1179 [02:07<04:54,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 350/1179, Loss: 0.3864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  34%|███▍      | 401/1179 [02:26<04:28,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 400/1179, Loss: 1.2715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  38%|███▊      | 451/1179 [02:44<04:12,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 450/1179, Loss: 0.5658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  40%|████      | 473/1179 [02:52<04:17,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  42%|████▏     | 501/1179 [03:02<03:54,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500/1179, Loss: 0.1548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  46%|████▌     | 539/1179 [03:16<03:51,  2.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  47%|████▋     | 551/1179 [03:20<03:37,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 550/1179, Loss: 0.4839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  50%|█████     | 595/1179 [03:36<03:32,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  51%|█████     | 601/1179 [03:38<03:23,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 600/1179, Loss: 0.3685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  52%|█████▏    | 611/1179 [03:42<03:25,  2.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  55%|█████▌    | 651/1179 [03:57<03:05,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 650/1179, Loss: 0.2113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  59%|█████▉    | 701/1179 [04:15<02:46,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 700/1179, Loss: 0.1357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  62%|██████▏   | 730/1179 [04:25<02:44,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  64%|██████▎   | 751/1179 [04:33<02:27,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 750/1179, Loss: 0.3441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  66%|██████▋   | 783/1179 [04:45<02:24,  2.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  68%|██████▊   | 801/1179 [04:51<02:10,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 800/1179, Loss: 0.1915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  69%|██████▊   | 808/1179 [04:54<02:14,  2.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  72%|███████▏  | 851/1179 [05:10<01:55,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 850/1179, Loss: 0.5033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  76%|███████▋  | 901/1179 [05:28<01:36,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 900/1179, Loss: 0.3031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  77%|███████▋  | 904/1179 [05:29<01:38,  2.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  80%|███████▉  | 939/1179 [05:42<01:27,  2.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  81%|████████  | 951/1179 [05:46<01:19,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 950/1179, Loss: 0.1609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  85%|████████▍ | 1001/1179 [06:04<01:01,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000/1179, Loss: 0.6392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  86%|████████▌ | 1014/1179 [06:09<01:00,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  87%|████████▋ | 1021/1179 [06:11<00:57,  2.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Training DistilBERT:  89%|████████▉ | 1051/1179 [06:22<00:44,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1050/1179, Loss: 0.4500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  93%|█████████▎| 1101/1179 [06:41<00:27,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1100/1179, Loss: 0.2638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT:  98%|█████████▊| 1151/1179 [06:59<00:09,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1150/1179, Loss: 0.6572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training DistilBERT: 100%|██████████| 1179/1179 [07:09<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT Average Training Loss: 0.4672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating DistilBERT:   1%|▏         | 6/409 [00:00<00:50,  8.03it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:   8%|▊         | 34/409 [00:04<00:44,  8.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  17%|█▋        | 70/409 [00:08<00:40,  8.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  29%|██▊       | 117/409 [00:14<00:35,  8.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  51%|█████▏    | 210/409 [00:25<00:24,  8.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  56%|█████▌    | 229/409 [00:28<00:21,  8.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  89%|████████▉ | 363/409 [00:44<00:05,  8.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  92%|█████████▏| 377/409 [00:46<00:03,  8.20it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  96%|█████████▋| 394/409 [00:48<00:01,  7.83it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT:  97%|█████████▋| 398/409 [00:48<00:01,  7.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Evaluating DistilBERT: 100%|██████████| 409/409 [00:50<00:00,  8.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT Validation Loss: 0.6071\n",
            "DistilBERT Dev Accuracy: 0.7073\n",
            "DistilBERT Dev F1: 0.7735\n",
            "New best DistilBERT model saved with F1 0.7735\n",
            "DistilBERT Best F1: 0.7735 at epoch 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Load the best RoBERTa model\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('/content/boolq_finetuned_roberta_best')\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('/content/boolq_finetuned_roberta_best')\n",
        "roberta_model.to(device)\n",
        "print(\"Best RoBERTa model loaded successfully.\")\n",
        "\n",
        "# Create test dataset with RoBERTa tokenizer\n",
        "test_dataset_roberta = BoolQDataset(test_df_clean, roberta_tokenizer, label_column='answer', is_test=True)\n",
        "test_loader_roberta = DataLoader(test_dataset_roberta, sampler=SequentialSampler(test_dataset_roberta), batch_size=8)\n",
        "\n",
        "# Predict on the test set with RoBERTa\n",
        "roberta_model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader_roberta, desc=\"Predicting with RoBERTa\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        outputs = roberta_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Add predictions to the test DataFrame\n",
        "test_df_clean['predicted_answer_roberta'] = [bool(pred) for pred in test_preds]\n",
        "print(\"Sample RoBERTa test predictions:\\n\", test_df_clean[['question', 'predicted_answer_roberta']].head())\n",
        "test_df_clean.to_csv('/content/boolq_test_predictions_roberta.csv', index=False)\n",
        "\n",
        "# Download the predictions\n",
        "from google.colab import files\n",
        "files.download('/content/boolq_test_predictions_roberta.csv')\n",
        "\n",
        "# Save to Google Drive\n",
        "drive_path = \"/content/drive/My Drive/Colab Notebooks/BoolQ_Project\"\n",
        "!cp /content/boolq_test_predictions_roberta.csv \"{drive_path}/\"\n",
        "print(\"RoBERTa test predictions saved and downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "9zj3LO1t_Ng7",
        "outputId": "3f81dc3c-347c-47a9-9b14-260c74b47cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RoBERTa model loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting with RoBERTa:   5%|▍         | 19/406 [00:04<01:23,  4.64it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa:  18%|█▊        | 72/406 [00:15<01:08,  4.87it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa:  24%|██▍       | 99/406 [00:21<01:03,  4.84it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa:  35%|███▍      | 141/406 [00:30<00:55,  4.81it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa:  39%|███▉      | 158/406 [00:33<00:51,  4.79it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa:  41%|████      | 165/406 [00:35<00:51,  4.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa:  82%|████████▏ | 333/406 [01:11<00:15,  4.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Predicting with RoBERTa: 100%|██████████| 406/406 [01:27<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample RoBERTa test predictions:\n",
            "                                             question  predicted_answer_roberta\n",
            "0  is the first series 20 euro note still legal t...                     False\n",
            "1  do the champions league winners get automatic ...                      True\n",
            "2                  can a bull snake kill a small dog                      True\n",
            "3                are all nba playoff games best of 7                      True\n",
            "4  can i use my train ticket on the tram in manch...                      True\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c24cc75d-6595-4a43-8359-58a495b938a3\", \"boolq_test_predictions_roberta.csv\", 2078771)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa test predictions saved and downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUfcQys9GAU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}